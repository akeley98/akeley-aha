\documentclass[12pt]{article}
\usepackage[letterpaper, portrait, margin=3cm]{geometry}

\usepackage{enumitem}
\usepackage{amsmath}
\usepackage{placeins}
\usepackage{graphicx}
\usepackage{caption}
\usepackage[parfill]{parskip}
\newcommand{\solution}[1]{{{\color{blue}{\bf Solution:} {#1}}}}
\usepackage[usenames,dvipsnames,svgnames,table,hyperref]{xcolor}


\begin{document}
\title{Contributions to Aetherling}
\author{David Akeley}
\maketitle

\begin{abstract}
This is a list of my contributions to the Aetherling project. I start
with a section summarizing tasks I worked on, then I expand on the
tasks in further sections.
\end{abstract}

\section{Summary}

\begin{enumerate}

\item Line Buffer Specifications

I proposed a new specification of Aetherling's line buffer node and
wrote a document (``The Line Buffer Manifesto'') describing the
benefits of this redesign. The previous line buffer design was hard to
parallelize due to difficult-to-satisfy constraints on its parameters'
divisibility, and also did not support downsampling (``stride'').
The redesign addresses these issues.

% Essential decision - move responsibility of bounds checking out
% of the line buffer; someone else downsteam will figure it out.
% For the purposes of timing, out-of-bounds pixels are considered
% valid outputs. Only later do we deal with the fact that they're
% actually garbage.

\item Functional Simulator

I wrote a functional simulator for Aetherling, which includes a
simulation of the intended behavior of the redesigned line
buffer. This allows the user to test the functionality of an
Aetherling DAG using pure Haskell code.

\item Demonstration Apps

I designed two demonstration Aetherling DAGs: Gaussian blur (7 $\times$
7 stencil) and mipmap generation. These DAGs can be tested using the
functional simulator along with a library (\texttt{ImageIO.hs}) for
converting between simulator values and png images on disk. These apps
also demonstrate the applications of the redesigned line buffer.

\item Simplifying Ops

An op is a node of an Aetherling DAG (along with its children,
sometimes). Previously there were multiple ways to express the same
functionality. I simplified the ops to minimize redundancy.

\item Helper Functions

I wrote some Haskell helper functions for creating ops and simple
patterns of ops. These helpers check for invalid parameters and
substitute for functionality lost through the op simplification.

\item Ready-Valid Meta-Op

By default, Aetherling pipelines are composed of ops representing
circuits with synchronous timing: they wait for a certain number of
warm-up cycles, then emit outputs on a repeating schedule (phase). I
designed a \texttt{ReadyValid} op that represents the idea of wrapping
a portion of an Aetherling DAG with a ready-valid interface.  I
modified the compose operators (\texttt{|\&|} and \texttt{|>>=|}) to
properly handle the `ReadyValid` op.  The user is prevented from
composing an op with ready-valid timing with one with synchronous
timing, and, when composing two ready-valid ops in sequence, the
throughput-matching behavior of the Aetherling type system is
suppressed.

\item ComposePar Retiming

Aetherling includes a \texttt{ComposePar} op that represents placing
circuits (child ops) in parallel. The user is not required to ensure
that each parallel path has the same latency (sequential latency --
the count of the number of register delays along a path). I wrote a
pass that walks an Aetherling DAG searching for \texttt{ComposePar}
ops, modifying its child ops if needed such that all paths have the
same latency. The pass finds an optimal solution that minimizes the
number of register bits added to the circuit.

\item Fractional Underutilization and Phase

In the Aetherling team, ``phase'' refers to the repeating pattern of
valid and garbage values input to/generated by a synchronously timed
op. For example, an op that generates 1 valid output every 3 cycles
would have an output phase of \texttt{[True, False, False]} (Two out
of every three cycles, the op generates garbage).

The choice of phase for an op with integer underutilization (1 valid
input/output per $X$ clock cycles) is obvious, but with fractional
underutilization ($X$ valid per $Y$ clocks), there are several
reasonable phase choices. When several such underutilized ops are
joined together, there's no reason to assume that their phase patterns
will match.

Since the Aetherling type system only exposes type and throughput
information to the user, it's vital that the system take care of phase
matching automatically. I proposed a scheme that assigns each
fractional utilization ratio a standardized phase. (The earlier phase
corresponds to $\frac{1}{3}$ utilization). This allows the complexity
of phase matching to be confined to one op in the system,
\texttt{SequenceArrayRepack}. Along with the ComposePar retiming pass,
this makes it so that users only have to be concerned with type and
throughput matching, allowing them to view phase and latency as
performance, rather than correctness, issues.

\item Tests Written

I gained a lot of experience writing tests as part of my work on the
Aetherling project. These tests include tests for the functional
simulator, tests for the hardware line buffer David Durst is
designing, tests for the compose operators (\texttt{|\&|} and
\texttt{|>>=|}), and tests for the \texttt{ComposePar} retiming passes.

\end{enumerate}

\section{Line Buffer Specifications}

\section{Functional Simulator}

\section{Demonstration Apps}

%% Talk about how writing apps led to realizations of what new line buffer needs?

\section{Simplifying Ops \& Helper Functions}

Previously, many Aetherling arithmetic ops (e.g. \texttt{Add}) took a
type parameter, which could be an array type. This meant that there
were multiple ways to express the same operation. For example, a bit
xor could be expressed as \texttt{XOr T\_Bit} or \texttt{Add T\_Bit},
and elementwise addition of two 4-arrays-of-int could be expressed
as \texttt{Add \$ T\_Array 4 T\_Int} (4-array type parameter) or
as a \texttt{MapOp 4} over a scalar \texttt{Add T\_Int}.

Since each op of the Aetherling Haskell IR will eventually need to be
implemented in hardware, it would be best to minimize unneeded
functionality as much as possible. I eliminated the type parameter
from arithmetic ops, splitting the op into two ops if bit and integer
versions are both needed (e.g. `And` for boolean $and$; `AndInts` for
bitwise $and$).

Since this change makes vectorized arithmetic harder to express
directly, I wrote some helper functions for expressing array types and
array operations. For example, a $16\times 16$ matrix of integers can
be expressed as \texttt{tInts[16,16]}, and an op performing
elementwise addition of two matrices can be expressed as
\texttt{addInts \$ tInts[16,16]}. Internally, the IR represents
vectorized ops as scalar ops wrapped by \texttt{MapOp}.
In this example, \texttt{addInts} will return

\texttt{MapOp 16 (MapOp 16 Add)}

The helper functions also serve the purpose of checking for invalid
parameters. For example, the function for creating a line buffer
op\footnote{Temporarily named \texttt{manifestoLineBuffer}.} checks that the
divisibility requirements are satisfied, and the \texttt{arrayReshape}
function, which creates an \texttt{ArrayReshape} op that reinterprets
inputs as a different type, checks that there's a reasonable mapping
between input and output types.\footnote{
Examples: \texttt{arrayReshape [tBits[2]] [T\_Bit, T\_Bit]} (conversion of
2-array-of-bit to two separate bits) is valid, while
\texttt{arrayReshape [tBits[3]] [T\_Bit]} is not since the input has
more bits than the output.}

\section{ReadyValid Op}

\section{ComposePar Retiming}

\section{Fractional Underutilization and Phase}

% I've tried to be extra verbose in this section because this concept
% so far has been really hard to explain. I know that verbose â‰  good
% explanation but it's the best I can do.

Originally Aetherling only supported the concept of integer
underutilization. After waiting for a certain number of warmup cycles,
synchronous ops could accept inputs or create outputs on a repeating
schedule of 1 valid input/output every \texttt{N} clock cycles, where
\texttt{N} is an integer.\footnote{Strictly speaking, Aetherling
  supported reciprocal-integer throughputs, which I refer to as
  integer underutilization since most ops can only support such
  throughputs through underutilization.}

The problem is that we wanted to lift the integer restriction on
underutilization and allow Aetherling ops to have any fractional
throughput (between 0 and 1) (``fractional underutilization''). The
benefit of the original integer underutilization scheme is that two
ops are guaranteed to work together when composed in sequence given
that their linked ports have the same throughput ($\frac{1}{N}$). As
long as the downstream op waits for $L$ cycles before accepting the
first input, where $L$ is the latency in clock cycles of the upstream
op, the timings of the two ops will match, with the upstream op
feeding input to the downstream op on cycles $L$, $L+N$, $L+2N$,
$L+3N$...

To maintain this simplicity with fractional underutilization, I
proposed that each fractional throughput should be assigned one
standardized phase pattern (repeating schedule of valid and garbage
inputs/outputs). The details of this assignment algorithm are not
important yet. The important points of this plan are that
\begin{enumerate}
\item Each op has one \texttt{sequentialLatency} value -- the
  difference in clock cycles between the time when its first input
  arrives and its first output is produced.\footnote{This is still
    well-defined if the op has multiple ports with varying
    throughputs. Each phase pattern starts with a valid input/output,
    so the initial batch of inputs are synchronized across all
    ports. Same with the initial batch of outputs. }
\item Each port of an op is assigned a phase pattern based on its
  throughput. (In Aetherling, this throughput is calculated by
  dividing \texttt{seqLen}, a property of the port, with \texttt{cps}
  (clocks per sequence), a property of the entire op).
\item Each output port of an op, in isolation, behaves like this: it
  emits garbage for \texttt{sequentialLatency} cycles, then cycles
  through its assigned phase pattern of valid and garbage outputs. (If
  the op is not in isolation, we'll have to wait additional cycles
  equal to the wait time for the first input to arrive -- this is
  equivalent to the sum of the upstream ops'
  \texttt{sequentialLatency}s). Note that a consequence of the earlier
  definition of \texttt{sequentialLatency} as the difference in time
  between the first input and first output, the phase pattern must
  start with a valid output.
\end{enumerate}

With these standardized phases, given that we properly delay
downstream ops, we can be certain that two ports' schedules will match
if their fractional throughputs match (checked by the Aetherling type
system), just as the case was for the earlier system that only
supported integer-reciprocal throughputs.

\subsection{Pipeline Example}

Consider the example pipeline \texttt{A |>>=| B |>>=| C}
(\texttt{|>>=|} means sequential compose, i.e. wire the outputs of
left op to inputs of right op). Suppose that \texttt{A} has
\texttt{sequentialLatency} 10, \texttt{B} has \texttt{sequentialLatency}
2, \texttt{C} has \texttt{sequentialLatency} 4, \texttt{B} has two output
ports $O_0$ and $O_1$ with throughputs $\frac{3}{5}$ and $\frac{1}{2}$
respectively, which matches input ports $I_0, I_1$ of \texttt{C}.
The standard phase for $\frac{3}{5}$ throughput is
\texttt{[True, True, False, True, False]},
and for $\frac{1}{2}$, \texttt{[True, False]}. (\texttt{True} denotes
a clock cycle with valid input/output; \texttt{False} denotes garbage).

Consider the output pattern of $O_0$. We'll start with 12 cycles of
garbage, since 12 is the sum of \texttt{A} and \texttt{B}'s
\texttt{sequentialLatency}s, then it'll create valid outputs on cycles
12, 13, 15, 17, 18, 20, 22... (since the $\frac{3}{5}$ phase indicates
the $0^{th}, 1^{st}, and 3^{rd}$ cycles in each 5-cycle pattern is
valid). Meanwhile, valid output will be sent through $O_1$ on cycles
12, 14, 16, 18, 20, 22...

This matches what \texttt{C}'s input ports $I_0$ and $I_1$
expect. They'll wait 12 cycles since 12 is the sum of the upstream
ops' \texttt{sequentialLatency}s, then, by looking up the same phase
pattern, \texttt{C} can reproduce the exact schedule that \texttt{B}
used. ($I_0$ expects input on cycles 12, 13, 15, 17...; $I_1$ on
cycles 12, 14, 16...).

Finally, note that the initial inputs and initial outputs are always
synchronized across ports. $O_0$ and $O_1$ start output on cycle 12,
and $I_0$ and $I_1$ start expecting input on cycle 12. This means that
the definition of \texttt{sequentialLatency} is well-defined. Using
the definition, we expect that each input port of \texttt{B} should
start accepting input on cycle 10, and each output port of \texttt{C}
should emit its first valid output on cycle 16.

\begin{center}
\includegraphics[width=1.0\linewidth]{Figures/phase.jpg}
Visualization of scheduling based on \texttt{sequentialLatency}
wait and repeating phase pattern.
\end{center}

\subsection{Phase Assignment Algorithm}

The throughput-to-phase assignment algorithm is based on the
\texttt{SequenceArrayRepack} op. This op converts between
sequences of arrays of different sizes. (In Aetherling terms, a
sequence is a stream of valid values delivered across multiple,
not-necessarily-contiguous clock cycles -- in the earlier
example, port $O_0$ delivers a 3-sequence of outputs on cycles
12, 13, and 15).

For example, a (fully utilized) \texttt{SequenceArrayRepack} that converts
from 1-sequences of 2-arrays to 2-sequences of 1-arrays would take a
valid 2-array input on cycles 0, 2, 4... and, using values unpacked from
the input arrays, generate a 1-array output on each clock cycle.

\begin{center}
% I'm so tired of figures ending up pages away and having to
% constantly fight with htb!!1! and friends so against all advice
% otherwise I'm ditching floating environments except when I
% personally feel like using one.
\includegraphics[width=1.0\linewidth]{Figures/repack.jpg}
\texttt{SequenceArrayRepack} repacking 2-arrays to 1-arrays.
Each sequence gets its own digit.
\end{center}

To determine the pattern for an $\frac{x}{Y}$ throughput, we figure out
what pattern would be most convenient for the input of a fully-utilized
\texttt{SequenceArrayRepack} converting $x$-sequences of $Y$-arrays
to $Y$-sequences of $x$-arrays. (This is a ``narrowing'' repack: since
\begin{equation*}
    \text{throughput}<1 \implies x<Y
\end{equation*}
the output arrays are narrower than
the input arrays). The repack is fully utilized, so the output comes
out at maximum rate: $X$ outputs over $X$ clock cycles (in Aetherling
terms, \texttt{cps=X}).

It would be most convenient for the \texttt{SequenceArrayRepack} if
each input array came in as soon as it was needed, but no sooner.
(This is convenient in the sense that it minimizes internal
buffering). Earlier, I set the $\frac{3}{5}$ throughput phase to
\texttt{[True, True, False, True, False]}. This pattern can be
reproduced by working through what a fully-utilized 3-sequence,
5-array to 5-sequence, 3-array \texttt{SequenceArrayRepack} input
schedule should be. Make a table with 5 columns, one for each clock
cycle 0-4 (fully-utilized $\implies$ \texttt{cps=5}), and have entries
for inputs, outputs, and leftovers in each column.

\begin{center}
\includegraphics[width=1.0\linewidth]{Figures/sorry-if-patronizing.jpg}
Visual aid. Fill in the chart for the next example.
\end{center}

On each clock cycle, one 3-array output must be produced. Represent
this by drawing 3 dots in the output cells. This $0^{th}$ cycle's
output must come from some input, so one 5-array input must come in
on the $0^{th}$ cycle, leaving 2 array entries left over that must
be stored in \texttt{SequenceArrayRepack}'s internal buffer. Draw
5 dots in the $0^{th}$ input cell and 2 dots in the $0^{th}$
leftover cell to represent this.

2 array entries are not enough to populate the $1^{st}$ cycle's 3-array
output, so on that cycle we must read in another 5-array input. By
cycle 1 then, the repack will have read in 10 and output 6 array entries,
leaving 4 stored internally. Draw 5, 3, and 4 dots in the first column
to represent this (input, output, leftover).

Now that there's 4 array entries buffered, we can produce another
3-array output on the $2^{nd}$ cycle without reading in another array.
By the earlier rule, we defer reading in another valid input for now,
producing the 3-array output using buffered inputs. Fill in 3 output
dots and 1 leftover dots in column 2 to represent this.

Fill in the last 2 columns using the same rule. There should be
input 5-arrays on cycles 0, 1, and 3, leading to the phase pattern
\texttt{[True, True, False, True, False]}. For each cycle 0-4, there
should be 2, 4, 1, 3, and finally 0 leftover entries.\footnote{
That there are 0 leftover entries at the end is notable: we would
just repeat if we continued the phase pattern for another 5 cycles.
If the throughput fraction $\frac{x}{Y}$ were not in reduced form, we
would still get the same phase (recall that phase repeats, so
\texttt{[True, False]}-repeated $\equiv$ \texttt{[True, False, True, False]}-%
repeated. Although I've explained this as a fraction-to-phase mapping,
in practice it's a mapping of \texttt{seqLen}$\equiv x$ and
\texttt{cps}$\equiv Y$ to phase, so it's not actually guaranteed
\textit{a priori} that two ops with matching fractional throughputs
will have matching phase if the $\frac{\text{seqLen}}{\text{cps}}$
fractions aren't in reduced form. That the phase is still the same
without reduced fractions is pretty important to this scheme, but
no one else seemed to be worried about this so it's in this footnote
here.

Actually, there's one last thing to note, which is that an
X-array to Y-array \texttt{SequenceArrayRepack} has the same schedule
(all else being equal) as an nX-array to nY-array repack, $n:$integer.
This is the other half of the phase being the same without reducing
fractions.

If you think about the dot chart from earlier, each input, output,
and leftover cell will have a multiple-of-$n$ number of dots. The
chart would be equivalent if we represented each group of $n$ dots
as a single dot, matching the original X-array to Y-array repack's chart,
and therefore its schedule.}
% I just had to point all that out.

\subsection{SequenceArrayRepack Example}

\subsection{Rationale}

\section{Test Writing}

The tests I designed include
\begin{enumerate}
\item

Haskell tests for the functional simulator I designed.

\item

Python tests for the hardware line buffer David Durst is
implementing in Magma based on ``The Line Buffer Manifesto''
specifications. These tests use the CoreIR simulator.

\item

Tests for the sequential compose (\texttt{|>>=|}) and parallel
compose (\texttt{|\&|}) Haskell Aetherling operators. These tests
check that the produced Haskell IR node is correct given valid
operands, and check that the operators reject invalid operands
(mismatched port types, mismatched synchronous and ready-valid
timing, and mismatched throughputs with synchronous timing).

\item Tests for the \texttt{ComposePar} retiming passes. There are
$18+$ test cases, some of which have compose ops nested several
layers deep meant to check for corner cases.
\end{enumerate}

\end{document}

